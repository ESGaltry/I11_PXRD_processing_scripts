{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eda25d24",
   "metadata": {},
   "source": [
    "# Data processing with Python "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184b0579",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c987ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import os\n",
    "import matplotlib as mpl\n",
    "import pandas as pd        \n",
    "import math\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install pyFAI\n",
    "import pyFAI\n",
    "from pyFAI.gui import jupyter\n",
    "\n",
    "!{sys.executable} -m pip install pybaselines\n",
    "from pybaselines import Baseline\n",
    "from pybaselines.utils import gaussian\n",
    "\n",
    "pi = math.pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4e0a33",
   "metadata": {},
   "source": [
    "# Load calibration file\n",
    "\n",
    "calibrated using pyFAI-calib2 tool\n",
    "\n",
    "pyFAI: https://pyfai.readthedocs.io/en/v2023.1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2ec700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the calibration .poni file \n",
    "calibration = pyFAI.load('C:/poni/file/location/calibration.poni')\n",
    "calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c749adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load mask for detector frame\n",
    "oneD_mask = np.load(\"C:/mask/file/location/mask.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c524449d",
   "metadata": {},
   "source": [
    "## Specify folder of frames to read in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aba7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder of nxs files to read in\n",
    "\n",
    "collection_dir = \"C:/collection/set/of/raw/nxs/files/location/\"\n",
    "count = 0\n",
    "file_nxs = []\n",
    "\n",
    "# Count the number of .nxs files in directory\n",
    "for files in os.listdir(collection_dir):\n",
    "    filename = os.fsdecode(files)\n",
    "    if filename.endswith(\".nxs\"):\n",
    "        file_nxs.append(filename)\n",
    "        count += 1\n",
    "print('File count:', count)\n",
    "print(file_nxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa0d167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making processing directory, change [0:-15] depending on you directory name to give it a reasonable name\n",
    "\n",
    "processing_folder_dir = collection_dir[0:-15]\n",
    "processing_folder = \"overall_processed_patterns/\"\n",
    "processing_path = os.path.join(processing_folder_dir, processing_folder)\n",
    "\n",
    "print(\"Processed pattern directory:\", processing_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb94557",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(processing_path):\n",
    "    os.makedirs(processing_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fc507d",
   "metadata": {},
   "source": [
    "## Integration using pyFAI module\n",
    "#### pyFAI: https://pyfai.readthedocs.io/en/v2023.1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f5965a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing files and 1D integrating with pyFAI\n",
    "\n",
    "frame_sized = np.zeros((count, 1441, 1440))\n",
    "int_patterns = []\n",
    "a = 0\n",
    "\n",
    "while a < count:\n",
    "    with h5py.File(collection_dir+file_nxs[a], 'r') as dat:\n",
    "        frame = np.array(dat[\"/entry1/pixium_hdf/data\"][()][:]) \n",
    "        frame_sized[a] = frame.reshape(frame.shape[1:])\n",
    "        int_patterns.append(calibration.integrate1d(frame_sized[a], 1000, unit=pyFAI.units.TTH_DEG, radial_range=[4,30], mask=oneD_mask))\n",
    "        a += 1\n",
    "\n",
    "two_theta = int_patterns[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d3dcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display all raw integrated patterns\n",
    "\n",
    "large = []\n",
    "\n",
    "# Create a new figure\n",
    "plt.figure(figsize=(14, 14))\n",
    "plt.xlabel('$2\\\\theta$ ($^{o}$)')\n",
    "plt.xlim(4, 20)\n",
    "\n",
    "plt.ylabel('Intensity')\n",
    "\n",
    "\n",
    "for i in range(count):\n",
    "    offset = i * 20  # Adjust the offset as needed\n",
    "    plt.plot(two_theta, int_patterns[i][1] + offset, label=format(file_nxs[i]))\n",
    "    large.append(max(int_patterns[i][1]+offset))\n",
    "\n",
    "huge = max(large)\n",
    "#plt.ylim(100,huge+20)\n",
    "plt.legend()\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d067fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pattern name to remove any ones displaying no diffraction\n",
    "\n",
    "remove = [61557 ,\n",
    "          61558 ,\n",
    "          61559 ,\n",
    "          61560 ,\n",
    "          61561 ,\n",
    "          61562 ,\n",
    "          61563]\n",
    "\n",
    "# or remove none    \n",
    "    \n",
    "remove = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0d344e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove files we don't want to process further\n",
    "\n",
    "remove_set = set(remove)\n",
    "\n",
    "filtered_file_list = [file_name for file_name in file_nxs if int(file_name.split('-')[-1].split('.')[0]) not in remove_set]\n",
    "\n",
    "filtered_no = len(filtered_file_list)\n",
    "print(\"Number of filtered files:\", filtered_no)\n",
    "print(\"Filtered file list:\", filtered_file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fafd8a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re integrate the chosen files \n",
    "\n",
    "\n",
    "filtered_frame_sized = np.zeros((count, 1441, 1440))\n",
    "filtered_int_patterns = []\n",
    "b = 0\n",
    "\n",
    "for b in range(filtered_no):\n",
    "    with h5py.File(collection_dir+filtered_file_list[b], 'r') as dat:\n",
    "        frame = np.array(dat[\"/entry1/pixium_hdf/data\"][()][:]) \n",
    "        filtered_frame_sized[b] = frame.reshape(frame.shape[1:])\n",
    "        filtered_int_patterns.append(calibration.integrate1d(filtered_frame_sized[b], 1000, unit=pyFAI.units.TTH_DEG, radial_range=[4,30], mask=oneD_mask))\n",
    "       # filtered_int_patterns.append(calibration.integrate1d(filtered_frame_sized[b], 1000, unit=pyFAI.units.TTH_DEG, radial_range=[4,30]))\n",
    "\n",
    "two_theta2 = filtered_int_patterns[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee1458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the final files to merge and process\n",
    "\n",
    "large2 = []\n",
    "\n",
    "# Create a new figure\n",
    "plt.figure(figsize=(14, 14))\n",
    "plt.xlabel('$2\\\\theta$ ($^{o}$)')\n",
    "plt.xlim(4, 20)\n",
    "\n",
    "plt.ylabel('Intensity')\n",
    "\n",
    "\n",
    "for j in range(filtered_no):\n",
    "    offset = j * 20  # Adjust the offset as needed\n",
    "    plt.plot(two_theta2, filtered_int_patterns[j][1] + offset, label=format(filtered_file_list[j]))\n",
    "    large2.append(max(filtered_int_patterns[j][1]+offset))\n",
    "\n",
    "huge2 = max(large2)\n",
    "plt.ylim(100,huge2+20)\n",
    "plt.legend()\n",
    "plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea3a816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summing and averaging the raw patterns\n",
    "\n",
    "raw_intensities = []\n",
    "\n",
    "for k in range(filtered_no):\n",
    "    raw_intensities.append(filtered_int_patterns[k][1])\n",
    "    \n",
    "added = np.sum(raw_intensities, axis=0)\n",
    "averaged_intensities = added/filtered_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e601d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the raw merged pattern\n",
    "\n",
    "plt.figure(figsize=(14, 14))\n",
    "plt.xlabel('$2\\\\theta$ ($^{o}$)')\n",
    "plt.xlim(4, 20)\n",
    "plt.ylim(80, 250)\n",
    "plt.ylabel('Intensity')\n",
    "\n",
    "plt.plot(two_theta2, averaged_intensities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e46119",
   "metadata": {},
   "source": [
    "# Baseline correction \n",
    "\n",
    "### using https://pybaselines.readthedocs.io/en/latest/ mor() baseline correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe650d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline correction set-up\n",
    "\n",
    "x = two_theta2\n",
    "y = averaged_intensities\n",
    "\n",
    "baseline_fitter = Baseline(x_data=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925af6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display baseline correction and edit half_window for a best fit\n",
    "\n",
    "half_window = 4\n",
    "\n",
    "plt.figure()\n",
    "plt.figure(figsize=(14, 14))\n",
    "plt.plot(x, y, label='data')\n",
    "plt.plot(x, baseline_fitter.mor(y, half_window=half_window)[0], label=f'half_window={half_window}')\n",
    "plt.xlabel('$2\\\\theta$ ($^{o}$)')\n",
    "plt.xlim(4, 20)\n",
    "#plt.ylim(80, 200)\n",
    "plt.ylabel('Intensity')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195f8a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline correction\n",
    "\n",
    "corrected_data = y - baseline_fitter.mor(y, half_window=half_window)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfaff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display all baseline correction stages\n",
    "\n",
    "plt.figure()\n",
    "plt.figure(figsize=(14, 14))\n",
    "plt.plot(x, y, label='data')\n",
    "plt.plot(x, baseline_fitter.mor(y, half_window=half_window)[0], label=f'half_window={half_window}')\n",
    "plt.plot(x, (corrected_data*20)+75, label='corrected_baseline')\n",
    "plt.xlabel('$2\\\\theta$ ($^{o}$)')\n",
    "plt.ylabel('Intensity /a.u.')\n",
    "plt.xlim(5, 20)\n",
    "plt.ylim(70, 200)\n",
    "plt.legend()\n",
    "#plt.savefig(processing_path + twoD_pattern_name_baseline_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cf8b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = modified_datasets_name + \"_fast_raw_merged_baselineCorr.xy\"\n",
    "print(data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eec71dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export final baseline corrected merged pattern for the collection set\n",
    "np.savetxt(processing_path+data_name, np.c_[x, corrected_data])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
